{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9f4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f279ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid size\n",
    "GRID_SIZE = 100\n",
    "OBSTACLE_DENSITY = 0.2  # 20% of cells are obstacles\n",
    "\n",
    "# Rewards\n",
    "GOAL_REWARD = 100\n",
    "STEP_PENALTY = -1\n",
    "OBSTACLE_PENALTY = -10\n",
    "\n",
    "# Define possible actions (up, down, left, right)\n",
    "ACTIONS = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6fa558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize grid with obstacles\n",
    "def create_grid():\n",
    "    grid = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    num_obstacles = int(GRID_SIZE * GRID_SIZE * OBSTACLE_DENSITY)\n",
    "    obstacles = random.sample([(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE)], num_obstacles)\n",
    "    for (i, j) in obstacles:\n",
    "        grid[i, j] = OBSTACLE_PENALTY\n",
    "    return grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4de6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start and goal points\n",
    "def set_start_goal(grid):\n",
    "    while True:\n",
    "        start = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "        goal = (random.randint(0, GRID_SIZE - 1), random.randint(0, GRID_SIZE - 1))\n",
    "        if start != goal and grid[start] != OBSTACLE_PENALTY and grid[goal] != OBSTACLE_PENALTY:\n",
    "            grid[goal] = GOAL_REWARD\n",
    "            return start, goal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da29c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP model with Value Iteration\n",
    "def value_iteration(grid, start, goal, gamma=0.9, theta=1e-4):\n",
    "    value_grid = np.zeros_like(grid)\n",
    "    policy_grid = np.zeros(grid.shape, dtype=int)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                if (i, j) == goal or grid[i, j] == OBSTACLE_PENALTY:\n",
    "                    continue\n",
    "                best_value = float('-inf')\n",
    "                best_action = None\n",
    "                for idx, action in enumerate(ACTIONS):\n",
    "                    ni, nj = i + action[0], j + action[1]\n",
    "                    if 0 <= ni < GRID_SIZE and 0 <= nj < GRID_SIZE:\n",
    "                        reward = grid[ni, nj] if grid[ni, nj] != OBSTACLE_PENALTY else OBSTACLE_PENALTY\n",
    "                        value = reward + gamma * value_grid[ni, nj]\n",
    "                        if value > best_value:\n",
    "                            best_value = value\n",
    "                            best_action = idx\n",
    "                delta = max(delta, abs(value_grid[i, j] - best_value))\n",
    "                value_grid[i, j] = best_value\n",
    "                policy_grid[i, j] = best_action\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return policy_grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e5b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning for the same environment\n",
    "def q_learning(grid, start, goal, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
    "    q_table = np.zeros((*grid.shape, len(ACTIONS)))\n",
    "    for episode in range(episodes):\n",
    "        state = start\n",
    "        while state != goal:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action_idx = random.randint(0, len(ACTIONS) - 1)\n",
    "            else:\n",
    "                action_idx = np.argmax(q_table[state[0], state[1]])\n",
    "            action = ACTIONS[action_idx]\n",
    "            next_state = (state[0] + action[0], state[1] + action[1])\n",
    "\n",
    "            if 0 <= next_state[0] < GRID_SIZE and 0 <= next_state[1] < GRID_SIZE:\n",
    "                reward = grid[next_state[0], next_state[1]]\n",
    "                if reward == OBSTACLE_PENALTY:\n",
    "                    reward = OBSTACLE_PENALTY  # Penalty for obstacle\n",
    "                    next_state = state\n",
    "                elif next_state == goal:\n",
    "                    reward = GOAL_REWARD\n",
    "                old_value = q_table[state[0], state[1], action_idx]\n",
    "                next_max = np.max(q_table[next_state[0], next_state[1]])\n",
    "                new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "                q_table[state[0], state[1], action_idx] = new_value\n",
    "                state = next_state\n",
    "    return q_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552b411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking function\n",
    "def benchmark(grid, policy_grid, q_table, start, goal):\n",
    "    vi_path, ql_path = [start], [start]\n",
    "    # Value Iteration Path\n",
    "    state = start\n",
    "    while state != goal:\n",
    "        action_idx = policy_grid[state]\n",
    "        action = ACTIONS[action_idx]\n",
    "        state = (state[0] + action[0], state[1] + action[1])\n",
    "        vi_path.append(state)\n",
    "        if len(vi_path) > GRID_SIZE ** 2:\n",
    "            break\n",
    "\n",
    "    # Q-Learning Path\n",
    "    state = start\n",
    "    while state != goal:\n",
    "        action_idx = np.argmax(q_table[state[0], state[1]])\n",
    "        action = ACTIONS[action_idx]\n",
    "        state = (state[0] + action[0], state[1] + action[1])\n",
    "        ql_path.append(state)\n",
    "        if len(ql_path) > GRID_SIZE ** 2:\n",
    "            break\n",
    "\n",
    "    return vi_path, ql_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4361b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_paths(grid, vi_path, ql_path, start, goal):\n",
    "    plt.imshow(grid, cmap='gray')\n",
    "    plt.plot(*zip(*vi_path), label=\"Value Iteration Path\", color=\"blue\")\n",
    "    plt.plot(*zip(*ql_path), label=\"Q-Learning Path\", color=\"green\")\n",
    "    plt.scatter(*start[::-1], color=\"red\", label=\"Start\")\n",
    "    plt.scatter(*goal[::-1], color=\"yellow\", label=\"Goal\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47cfcff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "grid = create_grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e69493",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, goal = set_start_goal(grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c9348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_grid = value_iteration(grid, start, goal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = q_learning(grid, start, goal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_path, ql_path = benchmark(grid, policy_grid, q_table, start, goal)\n",
    "plot_paths(grid, vi_path, ql_path, start, goal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
